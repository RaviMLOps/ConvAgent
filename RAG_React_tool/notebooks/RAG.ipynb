{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_core langchain_openai langchain_text_splitters\n",
    "!pip install chromadb\n",
    "!pip install langchainhub\n",
    "!pip install chromadb openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2 langchain sentence-transformers faiss-cpu openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76133992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "#pdf_path = r\"C:\\Users\\hc_ankit\\Documents\\IISc_Assigment_Quiz_track\\RAG\\GEP-Jan-2025.pdf\"\n",
    "pdf_path = \"/content/drive/My Drive/GEP-Jan-2025.pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    #print(pdf_path)\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(pdf_text[:500])  # Preview first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(f\"Sample Chunk: {chunks[100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample Chunk: {chunks[120]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Document objects\n",
    "splits = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee31853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#embedding_function = HuggingFaceEmbeddings(model_name=\"multilingual-e5-large-instruct\")\n",
    "\n",
    "# Store document chunks in ChromaDB\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)\n",
    "\n",
    "print(\"ChromaDB initialized with PDF embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9840810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt template for the question-answering system\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Always provide the citation for your answer.\n",
    "Always say \"Let me know if you need further help\" at the end of the answer.\n",
    "{context}   # This will be the context documents retrieved based on the question\n",
    "Question: {question}   # This will be the question being asked\n",
    "Helpful Answer:\"\"\"   # This is where the model's answer will be placed\n",
    "\n",
    "# Create the PromptTemplate instance with the specified variables and template\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Colab's userdata module to access stored secrets\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Fetching the OpenAI API key stored in Colab Secrets\n",
    "api_key = userdata.get('OPEN_API_KEY')  # <-- change this as per your secret's name\n",
    "\n",
    "# Storing the API key in the environment variables for global access\n",
    "os.environ['OPEN_API_KEY'] = api_key\n",
    "\n",
    "# Setting the OpenAI API key for the openai package to use\n",
    "openai.api_key = os.getenv('OPEN_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents( documents=splits,\n",
    "#                                    embedding = OpenAIEmbeddings(openai_api_key = api_key)\n",
    "#                                     )\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0,\n",
    "                 api_key = api_key )\n",
    "def format_docs(pdf_text):\n",
    "    return \"\\\\n\\\\n\".join(doc.page_content for doc in pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_chain.invoke(\"what are Outlook and risks of Regional perspectives\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# 4️⃣ Define the function for Gradio UI\n",
    "def answer_question(question):\n",
    "    response = rag_chain.invoke(question)  # 🔹 RAG pipeline to answer queries\n",
    "    return response  # Extracts the answer\n",
    "\n",
    "# 5️⃣ Build Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(label=\"Enter your question\"),  # User input\n",
    "    outputs=gr.Textbox(label=\"AI Answer\"),  # AI output\n",
    "    title=\"Gradio RAG Chatbot with ChromaDB & OpenAI\",\n",
    "    description=\"Ask any question, and the AI will retrieve relevant documents and generate an answer.\"\n",
    ")\n",
    "\n",
    "# 6️⃣ Launch the Gradio App\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
